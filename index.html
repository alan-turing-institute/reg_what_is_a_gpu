<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>What is a GPU?</title>

    <link rel="stylesheet" href="reveal.js/dist/reset.css">
    <link rel="stylesheet" href="reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="reveal.js/dist/theme/moon.css">

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css">

    <style>
        div.columns{display: flex; gap: min(4vw, 1.5em);}
        div.column{flex: auto; overflow-x: auto;}
    </style>

    <style>
        figure{font-size: medium;}
    </style>
</head>
<body>
<div class="reveal">
<div class="slides">
    <section data-markdown>
        # What is a GPU?

        ## A brief introduction for REG
    </section>
    <section data-markdown>
        <textarea data-template>
            ## Take Home Messages

            - Separate devices with own memory
            - Many ((tens of) thousands) of cores
            - Same operations applied on an input stream
            - Vector operations are **very** fast
            - Reductions are quite fast
            - Transferring data is **slow**
            <aside data-markdown class="notes">
                - These points are a simplification
                - I may say things I know not to be quite true, or simply be wrong
                - But this is all you need to know
                - GPUs are a separate device with their own processors and memory
                - GPUs have many (thousands) of cores
                - Many cores work together, performing the same operations on multiple pieces of data
                - Each core executes the same operations
                - Vector operations are **very** fast compared to CPU
                - Reductions are quite fast compared to CPU
                - Transferring data to and from the GPU is a common bottleneck
            </aside>
        </textarea>
    </section>
    <section>
        <section data-markdown>
            <textarea data-template>
                ## What is a GPU?
            </textarea>
        </section>
        <section>
            <h3>For Graphics</h3>
            <div class="columns">
                <div class="column" width="50%">
                    <figure>
                        <img src="img/geforce_2_MX400.jpg" alt="Nvidia RTX 4090">
                        <figcaption><a href="https://commons.wikimedia.org/wiki/File:Geforce_2_MX400_32mb_1.jpg">Thiemo Schuff</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0/de/deed.en">CC BY-SA 3.0 DE</a>, via Wikimedia Commons</figcaption>
                    </figure>
                </div>
                <div class="column" width="50%">
                    <figure>
                        <img src="img/rtx_4090.png" alt="Nvidia RTX 4090" width="80%">
                        <figcaption><a href="https://commons.wikimedia.org/wiki/File:NVIDIA_RTX_4090_Founders_Edition_-_Verpackung_(ZMASLO).png">ZMASLO</a>, <a href="https://creativecommons.org/licenses/by/3.0">CC BY 3.0</a>, via Wikimedia Commons</figcaption>
                    </figure>
                </div>
            </div>
            <aside data-markdown class="notes">
                - Graphical Processing Unit (GPU)
                - Originally used to offload graphical processing and rendering from the CPU
                - Handles 2D (shapes, sprites) and 3D graphics (polygons) acceleration
                - Over time, demand from computer games, 3D modelling, digital image processing drove an increase in power of GPUs
                - Example pictures showing 1. AGP/PCI power, no heat sink 2. Dedicated power, active cooling
                - **link to next slide**
            </aside>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### General-Purpose Computing on GPUs

                - Many graphics operations are vector operations
                - Interest in GPGPU grew as GPU throughput increased
                - Similar to past interest in vector processors
                - Support for IEEE floating point and double-precision
                <aside data-markdown class="notes">
                    - It just so happens that many graphics operations benefit from vectorisation
                    - GPUs have core which operate at much lower frequencies than CPUs
                    - However, for operations that can be vectorised they give greater performance due to their large number of cores being able to work together
                    - And so, as the computational throughput increased, interest in using GPUs for general-purpose computing arose
                    - Somewhat similar to vector processors seen in supercomputers in the 70s–80s
                    - Features of interest to general computation like higher precision IEEE types have been developed
                </aside>
            </textarea>
        </section>
        <section>
            <h3>How a GPU performs calculations</h3>

            <ul>
                <li>GPUs are separate devices, you <i>enqueue</i> kernels  on inputs</li>
                <li>Stream processors — kernels are applied to the whole data</li>
                <li>GPU coordinates many cores to apply the same kernel to elements of the input</li>
                <li>Outputs can be copied back to the host</li>
            </ul>
            <aside data-markdown class="notes">
                - GPUs are like a stream processor, coprocessor
                - Kernels are defined and applied to the _whole_ data
                - Host code enqueues a kernel on the inputs
                - The GPU coordinates its resources to solve the problem
                - Loops can be 'unrolled'. Many iterations happen at once (if there are no dependencies between iterations)
                - The host can retrieve the output when it is ready
            </aside>
        </section>
        <section>
            <h3>In relation to other Paradigms</h3>

            <div class="columns">
                <div class="column" width="50%">
                    Sequential
                    <pre><code class="julia" data-trim data-noescape>
                    a = rand(N)
                    b = rand(N)

                    for i = 1:N
                        a[i] = a[i] + b[i]
                    end
                    </code></pre>
                </div>
                <div class="column" width="50%">
                    Vectorisation and SIMD
                    <pre><code class="julia" data-trim data-noescape>
                    a = rand(N)
                    b = rand(N)

                    a .+= b
                    </code></pre>
                </div>
            </div>
            Stream
            Adapted from <a href="https://cuda.juliagpu.org/v4.0/tutorials/introduction/">CUDA.jl introduction</a> (<a href="#/cudajl-license">License</a>)
            <pre><code class="julia" data-trim data-noescape>
            using CUDA

            a_d = CUDA.rand(N)
            b_d = CUDA.rand(N)

            @cuda threads=256 gpu_add!(a_d, b_d)
            </code></pre>
            <aside data-markdown class="notes">
                - Simple example of element wise addition of two arrays in Julia
                - In conventional, sequential code each element is summed one at a time
                - Implying here that the CPU will do each addition one at a time, although a _good_ compiler will try to vectorise this
                - CPUs may support vector or SIMD operations where multiple additions will happen simulataneously
                - Compilers may translate the high level language to SIMD/vector CPU instructions
                - In the stream example
                  - The data is defined (`_d` indicates the memory is allocated on the device)
                  - The kernel (not shown here) is enqueued
                  - CUDA.jl hides a lot of the complexity here
            </aside>
        </section>
    </section>
    <section>
        <section data-markdown>
            <textarea data-template>
                ## Programming a GPU

                - Kernel and host code
                - The kernel will probably look like the body of a loop
                - The _same_ code will be executed on _every_ core
                - Use indices (thread and block id) to operate on different data
                <aside data-markdown class="notes">
                    - Writing lower level code for GPU, you will need to define the kernel and host code enqueuing the kernel
                    - You have to imagine the _same_ piece of code being run on _every_ core
                    - To make each core operate on different data, we use indices representing the dimensions of the problem
                    - Anyone who has written MPI code will find this familiar
                </aside>
            </textarea>
        </section>
        <section>
            <h3>Kernel 1: Vector Addition</h3>

            \[\mathbf{c}_{i}=\mathbf{a}_{i}+\mathbf{b}_{i}\]

            <div class="columns">
                <div class="column" width="50%">
                    <pre><code class="c" data-trim data-noescape>
                        __kernel void vadd(
                            __global const float* a,
                            __global const float* b,
                            __global float* c
                        ) {
                            // Get global dimension 0 (of 3)
                            int i = get_global_id(0);
                            // Add elements i of a and b
                            // store in c
                            c[i] = a[i] + b[i];
                        }
                    </code></pre>
                </div>
                <div class="column" width="50%">
                    <img src="img/vec_add.png" alt="Vector addition" width="300px">
                </div>
            </div>
            <aside data-markdown class="notes">
                - These examples are OpenCL as `global_id` simplifies getting a cores index
                - Core in GPUs are actually grouped
                  - GPU → Compute Units/Streaming Multiprocessors → cores
                - Problems are broken down
                    - In CUDA Grid → Block → Thread
                    - In OpenCL NDRange → Work Group → Work Item
                - Knowing the details is important for getting optimal performance as cores in the same group can share faster, local memory
            </aside>
        </section>
        <section>
            <h3>Kernel 2: Matrix Multiplication</h3>

            \[\mathbf{c}_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj}\]

            <div class="r-stack">
            <pre><code class="c" data-trim data-noescape>
                __kernel void matmul(
                    const int i_max, const int k_max,
                    const __global float* A, const __global float* B,
                    __global float* C
                ) {
                    // Get two global dimensions
                    const int i = get_global_id(0);
                    const int j = get_global_id(1);

                    // Calculate element ij of C
                    float acc = 0.0f;
                    for (int k=0; k < k_max; k++) {
                       // Column major elements
                       acc += A[k*i_max + i] * B[j*k_max + k];
                    }

                    // Store the result
                    C[i*i_max + j] = acc;
                }
            </code></pre>
            <aside data-markdown class="notes">
                - This algorithm is terrible
                - Order of 10^2 slower than library routines
                - Efficient matrix multiplication algorithms use many trick to optimise performance
            </aside>
        </section>
        <section>
            <h3>Kernel 3: Sum Reduction</h3>
            <div class="columns">
                <div class="column" width="30%">
                    \[b = \sum_{i=0}^{N} a_{i}\]
                </div>
                <div class="column" width="70%">
                    <img src="img/red_sum.png" alt="Vector addition" width="400px">
                </div>
            </div>
            <pre><code class="c" data-trim data-noescape>
            __kernel void sum(
                __global float* a, __local float* b,
                __global float* partials
            ) {
                int gid = get_global_id(0);

                int lid = get_local_id(0);
                int lsize = get_local_size(0);
                int wg = get_group_id(0);

                // Copy to local memory
                b[lid] = a[gid];
                barrier(CLK_LOCAL_MEM_FENCE);

                // Reduction within work group, sum is left in b[0]
                for (int stride=lsize>>1; stride>0; stride>>=1) {
                    if (lid < stride) {
                        b[lid] += b[lid+stride];
                    }
                    barrier(CLK_LOCAL_MEM_FENCE);
                }

                // Local thread 0 copies its work group's sum
                // to the result array
                if (lid == 0) {
                    partials[wg] = b[0];
                }
            }

            //
            </code></pre>
            <aside data-markdown class="notes">
                - Example shows work groups and local memory
                - Each core copies one element to local memory
                - Within each work group
                  - Repeatly divide the local array in half
                  - Sum top and bottom half
                - This way it takes the square root of the local size iterations to complete
                - Actually returns one element per working group
                - The results are in local memory, would need to copy back to global memory to get a scalar result
                - Need to pad the input so that each work group has a list length power of 2
            </aside>
        </section>
        <section data-markdown>
            <textarea data-template>
                ## Host Code

                - Identify devices
                - Create context(s) (object use to interact with a set of devices)
                - Optionally compile kernels
                - Allocate device/host memory
                - **Enqueue** kernels with given dimensions, inputs
                <aside data-markdown class="notes">
                    - The size and complexity of the host code will depend on the language you use
                    - For example, Python and Julia have modules which handle a lot of the process for the user
                    - In contrast, in C/C++ you will probably need a lot more boiler plate to
                    - It is in the host cost that you
                        - Identify devices
                        - Create context
                        - Allocate device/host memory
                        - Possibly compile kernels
                        - **Enqueue** kernels with given dimensions, inputs
                        - Notice how in the kernels we didn't specify the dimensions on the inputs
                        - This is because we assign the kernels to the whole input data
                </aside>
            </textarea>
        </section>
        <section>
            <h3>Host Code Example</h3>

            Adapted from <a href="https://github.com/inducer/pyopencl/blob/main/examples/demo.py">PyOpenCL demo example</a> (<a href="#/pyopencl-license">License</a>)

            <pre><code class="python" data-trim data-noescape>
            import numpy as np
            import pyopencl as cl

            # Create input on host
            a_h = np.random.rand(50000).astype(np.float32)
            b_h = np.random.rand(50000).astype(np.float32)

            ctx = cl.create_some_context()
            queue = cl.CommandQueue(ctx)

            # Allocate device memory for inputs
            mf = cl.mem_flags
            a_d = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR,
                            hostbuf=a_h)
            b_d = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR,
                            hostbuf=b_h)

            # Compile kernels
            prg = cl.Program(ctx, """
            __kernel void sum(
                __global const float *a_d, __global const float *b_d,
                __global float *res_d
            ) {
              int gid = get_global_id(0);
              res_d[gid] = a_d[gid] + b_d[gid];
            }
            """).build()

            # Allocate device memory for results
            res_d = cl.Buffer(ctx, mf.WRITE_ONLY, a_h.nbytes)

            # Enqueue sum kernel
            knl = prg.sum
            knl(queue, a_h.shape, None, a_d, b_d, res_d)
            # Kernel.__call__(queue, global_size, local_size, *args,
            #                 global_offset=None, wait_for=None,
            #                 g_times_l=False, allow_empty_ndrange=False)

            # Enqueue copying results back to host, blocking by default
            res_h = np.empty_like(a_h)
            cl.enqueue_copy(queue, res_h, res_d)
            </code></pre>
        </section>
        <section data-markdown>
            <textarea data-template>
                ## Higher Level Interfaces

                - You *probably* shouldn't write kernels
                - Are you likely to write a more performant and stable algorithm?

                #### Order of Preference

                1. Accelerated libraries (cuBLAS, numba, oneMKL)
                1. Embedded domain-specific languages (SYCL, OpenACC)
                1. Kernels (CUDA, HIP, OpenCL)
                <aside data-markdown class="notes">
                    - You are unlikely to be able to beat the performance of library kernels
                    - So, if you can use or combine library kernels to solve your problem that should be more performant and easier to write/maintain
                    - If you need to do something bespoke, eDSLs can get very close to kernel performance within one language
                    - Using an eDSL looks like OpenMP with directives (pragmas)
                    - If neither of those options work, you might want to write kernels
                    - However these will probably be harder to make robust and transferable
                </aside>
            </textarea>
        </section>
    </section>
    <section>
        <section data-markdown>
            <textarea data-template>
                ## Why do we Care?
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### Acceleration

                #### AI

                - AI (ML) involves many matrix operations
                - ML frameworks (Torch, Tensorflow, MXNet)

                #### Data Science

                - Analysis on large data sets
                - Dataframe and distributed computing libraries
                - GPGPU on workstations
                - HPC increasingly supporting interactive usage
                <aside data-markdown class="notes">
                    ## AI

                    - The most difficult parts of AI (ML) calculations are matrix operations
                    - Matrix operations can be greatly accelerated on GPUs compared to CPUs
                    - Popular ML frameworks support GPU acceleration out of the box
                    - Users don't need to deal with lower level interaction with GPUs
                    - Nvidia GPUs, through CUDA, tend to have the best support currently

                    ## Data Science

                    - Analysis on large data sets can benefit
                    - Vector operations and reductions in particular
                    - Dataframe and distributed computing libraries with GPU support
                    - Consumer GPUs are reasonably powerful and support GPGPU so you don't necessarily need to use HPC to benefit
                    - HPC systems are increasingly supporting interactive usage and notebook interfaces
                </aside>
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### Energy Efficiency

                - At peak, GPUs consume more power than CPUs
                - Typically higher FLOPS per Watt than CPU
                - Therefore, for a given calculation GPUs use energy
                - Data Centres increasingly focus on energy efficiency
                    - Recycling waste heat
                    - Lower impact cooling
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### HPC

                - Faster calculations, larger problems
                - Designed to maximising performance
                - Multiple GPUs per node
                - High speed interconnect between GPUs
                - High performance storage
                <aside data-markdown class="notes">
                    - There are multiple GPU focused HPC systems available for research
                    - They focus on optimising performance and will (almost certainly) perform much better than your laptop/workstation/cloud
                    - Feature high-end, data-centre GPUs
                    - Multiple GPUs can be used in a single job
                    - GPUs are connected to the host and to each other by high-speed fabric (remember that data transfer is often rate limited)
                    - High-speed, parallel storage appliances and on-node NVMe storage are common. Accelerates reading large data sets and writing checkpoints (model weights).
                </aside>
            </textarea>
        </section>
    </section>
    <section>
        <section>
            <h2>Licenses</h2>
    </section>
        <section id="pyopencl-license">
            <h3>PyOpenCL</h3>
            <pre><code class="txt" data-trim data-noescape>
            PyOpenCL is licensed to you under the MIT/X Consortium license:

            Copyright (c) 2009-13 Andreas Klöckner and Contributors.

            Permission is hereby granted, free of charge, to any person
            obtaining a copy of this software and associated documentation
            files (the "Software"), to deal in the Software without
            restriction, including without limitation the rights to use,
            copy, modify, merge, publish, distribute, sublicense, and/or sell
            copies of the Software, and to permit persons to whom the
            Software is furnished to do so, subject to the following
            conditions:

            The above copyright notice and this permission notice shall be
            included in all copies or substantial portions of the Software.

            THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
            EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
            OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
            NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
            HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
            WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
            FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
            OTHER DEALINGS IN THE SOFTWARE.
            </code></pre>
        </section>
        <section id="cudajl-license">
            <h3>CUDA.jl</h3>
            <pre><code class="txt" data-trim data-noescape>
            The CUDA.jl package is licensed under the MIT "Expat" License:

            Copyright (c) 2019-present: Julia Computing and other contributors

            Copyright (c) 2014-2018: Tim Besard

            Copyright (c) 2013: Dahua Lin

            All Rights Reserved.

            Permission is hereby granted, free of charge, to any person obtaining a copy
            of this software and associated documentation files (the "Software"), to deal
            in the Software without restriction, including without limitation the rights
            to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
            copies of the Software, and to permit persons to whom the Software is
            furnished to do so, subject to the following conditions:

            The above copyright notice and this permission notice shall be included in all
            copies or substantial portions of the Software.

            THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
            IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
            FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
            AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
            LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
            OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
            SOFTWARE.
            </code></pre>
        </section>
    </section>
</div>
</div>

<script src="reveal.js/dist/reveal.js"></script>
<script src="reveal.js/plugin/notes/notes.js"></script>
<script src="reveal.js/plugin/markdown/markdown.js"></script>
<script src="reveal.js/plugin/highlight/highlight.js"></script>
<script src="reveal.js/plugin/math/math.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,

        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
    });
</script>
</body>
</html>
