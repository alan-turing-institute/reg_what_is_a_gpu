<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>What is a GPU?</title>

    <link rel="stylesheet" href="reveal.js/dist/reset.css">
    <link rel="stylesheet" href="reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="reveal.js/dist/theme/moon.css">

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css">
    <style>
        div.columns{display: flex; gap: min(4vw, 1.5em);}
        div.column{flex: auto; overflow-x: auto;}
    </style>
</head>
<body>
<div class="reveal">
<div class="slides">
    <section data-markdown>
        # What is a GPU?

        ## A brief introduction for REG
    </section>
    <section data-markdown>
        <textarea data-template>
            ## Take Home Messages

            - Separate devices with own memory
            - Many (thousands) of cores
            - Cores do the same operations on an input stream
            <!-- - Each does the same thing -->
            - Vector operations are **very** fast
            - Reductions are quite fast
            - Transferring data is **slow**
            <aside data-markdown class="notes">
                - These points are a simplification
                - I may say things I know not to be quite true, or simply be wrong
                - But this is all you need to know
                - GPUs are a separate device with their own processors and memory
                - GPUs have many (thousands) of cores
                - Many cores work together, performing the same operations on multiple pieces of data
                - Each core executes the same operations
                - Vector operations are **very** fast compared to CPU
                - Reductions are quite fast compared to CPU
                - Transferring data to and from the GPU is a common bottleneck
            </aside>
        </textarea>
    </section>
    <section>
        <section data-markdown>
            <textarea data-template>
                ## What is a GPU?
            </textarea>
        </section>
        <section>
            <h3>For Graphics</h3>
            <div class="columns">
                <div class="column" width="50%">
                    <figure>
                        <img src="img/geforce_2_MX400.jpg" alt="Nvidia RTX 4090">
                        <figcaption><a href="https://commons.wikimedia.org/wiki/File:Geforce_2_MX400_32mb_1.jpg">Thiemo Schuff</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0/de/deed.en">CC BY-SA 3.0 DE</a>, via Wikimedia Commons</figcaption>
                    </figure>
                </div>
                <div class="column" width="50%">
                    <figure>
                        <img src="img/rtx_4090.png" alt="Nvidia RTX 4090" width="80%">
                        <figcaption><a href="https://commons.wikimedia.org/wiki/File:NVIDIA_RTX_4090_Founders_Edition_-_Verpackung_(ZMASLO).png">ZMASLO</a>, <a href="https://creativecommons.org/licenses/by/3.0">CC BY 3.0</a>, via Wikimedia Commons</figcaption>
                    </figure>
                </div>
            </div>
            <aside data-markdown class="notes">
                - Graphical Processing Unit (GPU)
                - Originally used to offload graphical processing and rendering from the CPU
                - Handles 2D (shapes, sprites) and 3D graphics (polygons) acceleration
                - Over time, demand from computer games, 3D modelling, digital image processing drove an increase in power of GPUs
                - Example pictures showing 1. AGP/PCI power, no heat sink 2. Dedicated power, active cooling
                - **link to next slide**
            </aside>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### General-Purpose Computing on GPUs

                - Many graphics operations are vector operations
                - Interest in GPGPU grew as GPU throughput increased
                - Similar to past interest in vector processors
                - Support for IEEE floating point and double-precision
                <aside data-markdown class="notes">
                    - It just so happens that many graphics operations benefit from vectorisation
                    - GPUs have core which operate at much lower frequencies than CPUs
                    - However, for operations that can be vectorised they give greater performance due to their large number of cores being able to work together
                    - And so, as the computational throughput increased, interest in using GPUs for general-purpose computing arose
                    - Somewhat similar to vector processors seen in supercomputers in the 70s–80s
                    - Features of interest to general computation like higher precision IEEE types have been developed
                </aside>
            </textarea>
        </section>
        <section>
            <h3>How a GPU performs calculations</h3>

            <ul>
                <li>GPUs are separate devices, you <i>enqueue</i> kernels  on inputs</li>
                <li>Stream processors — kernels are applied to the whole data</li>
                <li>GPU coordinates many cores to apply the same kernel to elements of the input</li>
                <li>Outputs can be copied back to the host</li>
            </ul>
            <aside data-markdown class="notes">
                - GPUs are like a stream processor, coprocessor
                - Kernels are defined and applied to the _whole_ data
                - Host code enqueues a kernel on the inputs
                - The GPU coordinates its resources to solve the problem
                - Loops can be 'unrolled'. Many iterations happen at once (if there are no dependencies between iterations)
                - The host can retrieve the output when it is ready
            </aside>
        </section>
        <section>
            <h3>In relation to other Paradigms</h3>

            <div class="columns">
                <div class="column" width="50%">
                    Sequential
                    <pre><code class="julia" data-trim data-noescape>
                    a = rand(N)
                    b = rand(N)

                    for i = 1:N
                        a[i] = a[i] + b[i]
                    end
                    </code></pre>
                </div>
                <div class="column" width="50%">
                    Vectorisation and SIMD
                    <pre><code class="julia" data-trim data-noescape>
                    a = rand(N)
                    b = rand(N)

                    a .+= b
                    </code></pre>
                </div>
            </div>
            Stream
            <pre><code class="julia" data-trim data-noescape>
            using CUDA

            a_d = CUDA.rand(N)
            b_d = CUDA.rand(N)

            @cuda threads=256 gpu_add!(a_d, b_d)
            </code></pre>

            Adapted from <a href="https://cuda.juliagpu.org/v4.0/tutorials/introduction/">CUDA.jl introduction</a>
            <aside data-markdown class="notes">
                - Simple example of element wise addition of two arrays in Julia
                - In conventional, sequential code each element is summed one at a time
                - Implying here that the CPU will do each addition one at a time, although a _good_ compiler will try to vectorise this
                - CPUs may support vector or SIMD operations where multiple additions will happen simulataneously
                - Compilers may translate the high level language to SIMD/vector CPU instructions
                - In the stream example
                  - The data is defined (`_d` indicates the memory is allocated on the device)
                  - The kernel (not shown here) is enqueued
                  - CUDA.jl hides a lot of the complexity here
            </aside>
        </section>
    </section>
    <section>
        <section data-markdown>
            <textarea data-template>
                ## Programming a GPU

                - Kernel and host code
                - The kernel will probably look like the body of a loop
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ## Device Kernel 1: Vector Add
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ## Device Kernel 2: Matrix Multiplication
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ## Host Code

                - Identify devices
                - Create context
                - Allocate device/host memory
                - **Enqueue** kernels with given dimensions, inputs
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ## Higher Level Interface

                - You *probably* shouldn't write kernels
                - Are you likely to write a more performant and stable algorithm?
                - Order of preference

                1. Accelerated libraries (_e.g._ cuBLAS, numba, oneMKL, _etc._)
                1. Embedded domain-specific languages (SYCL, OpenACC)
                1. Kernels (CUDA, HIP, OpenCL)
            </textarea>
        </section>
    </section>
    <section>
        <section data-markdown>
            <textarea data-template>
                ## Why do we Care?
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### Accelerating AI and ML
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### Energy Efficiency

                - Typically higher FLOPS per Watt than CPU
                - Data Centres
                - Energy efficiency
                - Recycling waste heat
                - Lower impact cooling
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### Data Centres

                - Maximising performance
                - Multiple GPUs per node
                - High speed interconnect between GPUs
                - High performance storage
            </textarea>
        </section>
    </section>
</div>
</div>

<script src="reveal.js/dist/reveal.js"></script>
<script src="reveal.js/plugin/notes/notes.js"></script>
<script src="reveal.js/plugin/markdown/markdown.js"></script>
<script src="reveal.js/plugin/highlight/highlight.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,

        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
    });
</script>
</body>
</html>
