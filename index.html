<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>What is a GPU?</title>

    <link rel="stylesheet" href="reveal.js/dist/reset.css">
    <link rel="stylesheet" href="reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="reveal.js/dist/theme/moon.css">

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css">

    <style>
        div.columns{display: flex; gap: min(4vw, 1.5em);}
        div.column{flex: auto; overflow-x: auto;}
    </style>

    <style>
        figure{font-size: medium;}
    </style>
</head>
<body>
<div class="reveal">
<div class="slides">
    <section data-markdown>
        # What is a GPU?

        ## A brief introduction for REG
    </section>
    <section data-markdown>
        <textarea data-template>
            ## Take Home Messages

            - Separate devices with own memory
            - Many ((tens of) thousands) of cores
            - Same operations applied on an input stream
            - Vector operations are **very** fast
            - Reductions are quite fast
            - Transferring data is **slow**
            <aside data-markdown class="notes">
                - These points are a simplification
                - I may say things I know not to be quite true, or simply be wrong
                - But this is all you need to know
                - GPUs are a separate device with their own processors and memory
                - GPUs have many (thousands) of cores
                - Many cores work together, performing the same operations on multiple pieces of data
                - Each core executes the same operations
                - Vector operations are **very** fast compared to CPU
                - Reductions are quite fast compared to CPU
                - Transferring data to and from the GPU is a common bottleneck
            </aside>
        </textarea>
    </section>
    <section>
        <section data-markdown>
            <textarea data-template>
                ## What is a GPU?
            </textarea>
        </section>
        <section>
            <h3>For Graphics</h3>
            <div class="columns">
                <div class="column" width="50%">
                    <figure>
                        <img src="img/geforce_2_MX400.jpg" alt="Nvidia RTX 4090">
                        <figcaption><a href="https://commons.wikimedia.org/wiki/File:Geforce_2_MX400_32mb_1.jpg">Thiemo Schuff</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0/de/deed.en">CC BY-SA 3.0 DE</a>, via Wikimedia Commons</figcaption>
                    </figure>
                </div>
                <div class="column" width="50%">
                    <figure>
                        <img src="img/rtx_4090.png" alt="Nvidia RTX 4090" width="80%">
                        <figcaption><a href="https://commons.wikimedia.org/wiki/File:NVIDIA_RTX_4090_Founders_Edition_-_Verpackung_(ZMASLO).png">ZMASLO</a>, <a href="https://creativecommons.org/licenses/by/3.0">CC BY 3.0</a>, via Wikimedia Commons</figcaption>
                    </figure>
                </div>
            </div>
            <aside data-markdown class="notes">
                - Graphical Processing Unit (GPU)
                - Originally used to offload graphical processing and rendering from the CPU
                - Handles 2D (shapes, sprites) and 3D graphics (polygons) acceleration
                - Over time, demand from computer games, 3D modelling, digital image processing drove an increase in power of GPUs
                - Example pictures showing 1. AGP/PCI power, no heat sink 2. Dedicated power, active cooling
                - **link to next slide**
            </aside>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### General-Purpose Computing on GPUs

                - Many graphics operations are vector operations
                - Interest in GPGPU grew as GPU throughput increased
                - Similar to past interest in vector processors
                - Support for IEEE floating point and double-precision
                <aside data-markdown class="notes">
                    - It just so happens that many graphics operations benefit from vectorisation
                    - GPUs have core which operate at much lower frequencies than CPUs
                    - However, for operations that can be vectorised they give greater performance due to their large number of cores being able to work together
                    - And so, as the computational throughput increased, interest in using GPUs for general-purpose computing arose
                    - Somewhat similar to vector processors seen in supercomputers in the 70s–80s
                    - Features of interest to general computation like higher precision IEEE types have been developed
                </aside>
            </textarea>
        </section>
        <section>
            <h3>How a GPU performs calculations</h3>

            <ul>
                <li>GPUs are separate devices, you <i>enqueue</i> kernels  on inputs</li>
                <li>Stream processors — kernels are applied to the whole data</li>
                <li>GPU coordinates many cores to apply the same kernel to elements of the input</li>
                <li>Outputs can be copied back to the host</li>
            </ul>
            <aside data-markdown class="notes">
                - GPUs are like a stream processor, coprocessor
                - Kernels are defined and applied to the _whole_ data
                - Host code enqueues a kernel on the inputs
                - The GPU coordinates its resources to solve the problem
                - Loops can be 'unrolled'. Many iterations happen at once (if there are no dependencies between iterations)
                - The host can retrieve the output when it is ready
            </aside>
        </section>
        <section>
            <h3>In relation to other Paradigms</h3>

            <div class="columns">
                <div class="column" width="50%">
                    Sequential
                    <pre><code class="julia" data-trim data-noescape>
                    a = rand(N)
                    b = rand(N)

                    for i = 1:N
                        a[i] = a[i] + b[i]
                    end
                    </code></pre>
                </div>
                <div class="column" width="50%">
                    Vectorisation and SIMD
                    <pre><code class="julia" data-trim data-noescape>
                    a = rand(N)
                    b = rand(N)

                    a .+= b
                    </code></pre>
                </div>
            </div>
            Stream
            <pre><code class="julia" data-trim data-noescape>
            using CUDA

            a_d = CUDA.rand(N)
            b_d = CUDA.rand(N)

            @cuda threads=256 gpu_add!(a_d, b_d)
            </code></pre>

            Adapted from <a href="https://cuda.juliagpu.org/v4.0/tutorials/introduction/">CUDA.jl introduction</a>
            <aside data-markdown class="notes">
                - Simple example of element wise addition of two arrays in Julia
                - In conventional, sequential code each element is summed one at a time
                - Implying here that the CPU will do each addition one at a time, although a _good_ compiler will try to vectorise this
                - CPUs may support vector or SIMD operations where multiple additions will happen simulataneously
                - Compilers may translate the high level language to SIMD/vector CPU instructions
                - In the stream example
                  - The data is defined (`_d` indicates the memory is allocated on the device)
                  - The kernel (not shown here) is enqueued
                  - CUDA.jl hides a lot of the complexity here
            </aside>
        </section>
    </section>
    <section>
        <section data-markdown>
            <textarea data-template>
                ## Programming a GPU

                - Kernel and host code
                - The kernel will probably look like the body of a loop
                - The _same_ code will be executed on _every_ core
                - Use indices (thread and block id) to operate on different data
                <aside data-markdown class="notes">
                    - Writing lower level code for GPU, you will need to define the kernel and host code enqueuing the kernel
                    - You have to imagine the _same_ piece of code being run on _every_ core
                    - To make each core operate on different data, we use indices representing the dimensions of the problem
                </aside>
            </textarea>
        </section>
        <section>
            <h3>Kernel 1: Vector Addition</h3>

            \[\mathbf{c}_{i}=\mathbf{a}_{i}+\mathbf{b}_{i}\]

            <pre><code class="c" data-trim data-noescape>
                __kernel void vadd(
                    __global const float* a,
                    __global const float* b,
                    __global float* c
                ) {
                    // Get global dimension 0 (of 3)
                    int gid = get_global_id(0);
                    // Add elements gid of a and b, store in c
                    c[gid] = a[gid] + b[gid];
                }
            </code></pre>
            <aside data-markdown class="notes">
                - These examples are OpenCL as `global_id` simplifies getting a cores index
                - Core in GPUs are actually grouped
                  - GPU → Compute Units/Streaming Multiprocessors → cores
                - Problems are broken down
                    - In CUDA Grid → Block → Thread
                    - In OpenCL NDRange → Work Group → Work Item
                - Knowing the details is important for getting optimal performance as cores in the same group can share faster, local memory
            </aside>
        </section>
        <section>
            <h3>Kernel 2: Matrix Multiplication</h3>

            \[\mathbf{c}_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj}\]

            <div class="r-stack">
            <pre class="fragment fade-out"><code class="c" data-trim data-noescape>
                __kernel void matmul(
                    const int i_max, const int k_max,
                    const __global float* A, const __global float* B,
                    __global float* C
                ) {
                    // Get two global dimensions
                    const int i = get_global_id(0);
                    const int j = get_global_id(1);

                    // Calculate element ij of C
                    float acc = 0.0f;
                    for (int k=0; k < k_max; k++) {
                       // Column major elements
                       acc += A[k*i_max + i] * B[j*k_max + k];
                    }

                    // Store the result
                    C[i*i_max + j] = acc;
                }
            </code></pre>
            <p class="fragment current-visible">
                <ul>
                    <li>This is a poor algorithm</li>
                    <li>Order of 10<sup>2</sup> slower than library routines</li>
                </ul>
            </p>
            </div>
            <aside data-markdown class="notes">
                - This algorithm is terrible
                - Efficient matrix multiplication algorithms use many trick to optimise performance
            </aside>
        </section>
        <section>
            <h3>Kernel 3: Sum Reduction</h3>

            \[b = \sum_{i=0}^{N} a_{i}\]
            <pre><code class="c" data-trim data-noescape>
            __kernel void sum(__global float* a, __local float* b,
                  __global float* partials) {
            int gid = get_global_id(0);

            int lid = get_local_id(0);
            int lsize = get_local_size(0);
            int wg = get_group_id(0);

            // Copy to local memory
            b[lid] = a[gid];
            barrier(CLK_LOCAL_MEM_FENCE);

            // Reduction within work group, sum is left in b[0]
            for (int stride=lsize>>1; stride>0; stride>>=1) {
                if (lid < stride) {
                    b[lid] += b[lid+stride];
                }
                barrier(CLK_LOCAL_MEM_FENCE);
            }

            // Local thread 0 copies its work group sum to the result array
            if (lid == 0) {
                partials[wg] = b[0];
            }
        }
        </code></pre>
        <aside data-markdown class="notes">
            - Actually returns one element per working group
            - Need to pad the input to be a power of 2
        </aside>
        </section>
        <section data-markdown>
            <textarea data-template>
                ## Host Code

                - Identify devices
                - Create context
                - Allocate device/host memory
                - **Enqueue** kernels with given dimensions, inputs
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ## Higher Level Interface

                - You *probably* shouldn't write kernels
                - Are you likely to write a more performant and stable algorithm?
                - Order of preference

                1. Accelerated libraries (_e.g._ cuBLAS, numba, oneMKL, _etc._)
                1. Embedded domain-specific languages (SYCL, OpenACC)
                1. Kernels (CUDA, HIP, OpenCL)
            </textarea>
        </section>
    </section>
    <section>
        <section data-markdown>
            <textarea data-template>
                ## Why do we Care?
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### Accelerating AI and ML
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### Energy Efficiency

                - Typically higher FLOPS per Watt than CPU
                - Data Centres
                - Energy efficiency
                - Recycling waste heat
                - Lower impact cooling
            </textarea>
        </section>
        <section data-markdown>
            <textarea data-template>
                ### Data Centres

                - Maximising performance
                - Multiple GPUs per node
                - High speed interconnect between GPUs
                - High performance storage
            </textarea>
        </section>
    </section>
</div>
</div>

<script src="reveal.js/dist/reveal.js"></script>
<script src="reveal.js/plugin/notes/notes.js"></script>
<script src="reveal.js/plugin/markdown/markdown.js"></script>
<script src="reveal.js/plugin/highlight/highlight.js"></script>
<script src="reveal.js/plugin/math/math.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,

        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
    });
</script>
</body>
</html>
